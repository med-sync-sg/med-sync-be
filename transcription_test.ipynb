{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transcription Test**\n",
    "This notebook is designed to test speech-to-text **without dependencies** on FastAPI (`app.py`) and the database (`session.py`).\n",
    "- Uses **Wav2Vec2** for transcription.\n",
    "- Simulates **real-time audio buffering**.\n",
    "- Processes **dummy audio data**.\n",
    "- **No database or NLP dependencies.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pyaudio\n",
    "import torch\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import os\n",
    "from pyctcdecode import build_ctcdecoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock functions to remove dependency on FastAPI and database\n",
    "def process_text(text):\n",
    "    return text  # Just return raw text for standalone execution\n",
    "\n",
    "class AudioCollector:\n",
    "    \"\"\"\n",
    "    A class to accumulate audio data and process it using Wav2Vec2 for speech-to-text.\n",
    "    \"\"\"\n",
    "    _instance = None\n",
    "    \n",
    "    def __new__(cls):\n",
    "        \n",
    "        '''\n",
    "        facebook/wav2vec2-base-960h - Base model, trained on 960 hrs of English speech\n",
    "        facebook/wav2vec2-large-960h - Large model, trained on 960 hrs of English speech\n",
    "        facebook/wav2vec2-large-xlsr-53 - Large model, trained on 53 languages (multilingual)\n",
    "        '''\n",
    "        MODEL_ID = \"facebook/wav2vec2-base-960h\" # {facebook/wav2vec2-large-960h, facebook/wav2vec2-large-xlsr-53}\n",
    "        \n",
    "        if cls._instance is None:\n",
    "            print(\"Initializing AudioCollector...\")\n",
    "            cls._instance = super(AudioCollector, cls).__new__(cls)\n",
    "            cls._instance.audio_buffer = bytearray()\n",
    "            #cls._instance.session_audio = bytearray()       # accumulates full session audio for later processing\n",
    "\n",
    "            # cls._instance.model = whisper.load_model(\"medium\")  # or \"base\" for faster processing\n",
    "            cls._instance.processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "            cls._instance.model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "            cls._instance.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "            cls._instance.model.to(cls._instance.device)\n",
    "        return cls._instance\n",
    "    \n",
    "    def add_chunk(self, chunk: bytes):\n",
    "        \"\"\"Add new PCM data to both the real-time and session buffers.\"\"\"\n",
    "        self.audio_buffer.extend(chunk)\n",
    "        #self.session_audio.extend(chunk)\n",
    "\n",
    "    def reset_buffer(self):\n",
    "        \"\"\"Reset only the real-time audio buffer (keeping session audio intact).\"\"\"\n",
    "        self.audio_buffer = bytearray()\n",
    "\n",
    "    def transcribe_audio_segment(self):\n",
    "        \"\"\"\n",
    "        Preprocesses the audio and runs inference using the XLS-R model.\n",
    "        Returns the transcription as text.\n",
    "        \"\"\"\n",
    "        \n",
    "        \"\"\"Convert the buffer to text using Wav2Vec2.\"\"\"\n",
    "        if len(self.audio_buffer) == 0:\n",
    "            return \"No audio data to transcribe.\"\n",
    "\n",
    "        sample_rate = 16000\n",
    "        sample_width = 2  # bytes per sample\n",
    "        #frame_duration_ms = 20\n",
    "        #frame_size = int(sample_rate * (frame_duration_ms / 1000.0) * sample_width)  # ~640 bytes\n",
    "\n",
    "        samples = np.frombuffer(self.audio_buffer, dtype=np.int16).astype(np.float32) / 32768.0\n",
    "\n",
    "        if sample_rate != 16000:\n",
    "                print(f\"Warning: Sample rate is {sample_rate}Hz. Expected 16000Hz.\")\n",
    "\n",
    "        input_values = self.processor(samples, sampling_rate=sample_rate, return_tensors=\"pt\").input_values\n",
    "        input_values = input_values.to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = self.model(input_values).logits\n",
    "\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = self.processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "        self.reset_buffer()\n",
    "        return transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcription result: \n"
     ]
    }
   ],
   "source": [
    "collector = AudioCollector()\n",
    "\n",
    "# Simulate a chunk of raw PCM audio (normally this would be captured from a microphone)\n",
    "dummy_audio_chunk = np.random.randint(-32768, 32767, 16000, dtype=np.int16).tobytes()\n",
    "\n",
    "collector.add_chunk(dummy_audio_chunk)\n",
    "transcription = collector.transcribe_audio_segment()\n",
    "print(\"Transcription result:\", transcription)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Transcription Test with Different Models**\n",
    "\n",
    "Models we are using are\n",
    "- **facebook/wav2vec2-base-960h** - Base model, trained on 960 hrs of English speech\n",
    "- **facebook/wav2vec2-large-960h** - Large model, trained on 960 hrs of English speech\n",
    "- **facebook/wav2vec2-large-xlsr-53** - Large model, trained on 53 languages (multilingual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File found! Trying to open it...\n",
      "File opened successfully! Sample rate: 16000\n",
      "Loading file: /Users/munhuikim/Desktop/med-sync-be/test_audios/test01_20s.wav\n",
      "Transcription Result: DANCING IN THE MASQUERADE IDLE TRUTH AN PLAIN SIGHT JADED POP ROLL CLICK SHOT WHO WILL I BE TO DAY OR NOT BUT SUCH A TIDE AS MOVING SEEMS ASLEEP TOO FULL FOR SOUND AND POAM WHEN THAT WHICH DREW FROM OUT THE BOUNDLESS DEEP TURNS AGAIN HOME TWILIGHT AND EVENING BELL AND AFTER THAT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import soundfile as sf\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "\n",
    "'''\n",
    "facebook/wav2vec2-base-960h - Base model, trained on 960 hrs of English speech\n",
    "facebook/wav2vec2-large-960h - Large model, trained on 960 hrs of English speech\n",
    "facebook/wav2vec2-large-xlsr-53 - Large model, trained on 53 languages (multilingual)\n",
    "'''\n",
    "\n",
    "MODEL_ID = \"facebook/wav2vec2-base-960h\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(MODEL_ID)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(MODEL_ID)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "def transcribe_audio(file_path):\n",
    "    print(f\"Loading file: {file_path}\")\n",
    "\n",
    "    audio_samples, sample_rate = sf.read(file_path)\n",
    "\n",
    "    if sample_rate != 16000:\n",
    "        print(f\"Warning: Sample rate is {sample_rate}Hz. Expected 16000Hz.\")\n",
    "\n",
    "    input_values = processor(audio_samples, sampling_rate=16000, return_tensors=\"pt\").input_values\n",
    "    input_values = input_values.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_values).logits\n",
    "\n",
    "    predicted_ids = torch.argmax(logits, dim=-1)\n",
    "    transcription = processor.batch_decode(predicted_ids)[0]\n",
    "\n",
    "    return transcription\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import soundfile as sf\n",
    "\n",
    "    file_path = \"/Users/munhuikim/Desktop/med-sync-be/test_audios/test01_20s.wav\"\n",
    "    if os.path.exists(file_path):\n",
    "        print(\"File found! Trying to open it...\")\n",
    "        try:\n",
    "            audio_samples, sample_rate = sf.read(file_path)\n",
    "            print(\"File opened successfully! Sample rate:\", sample_rate)\n",
    "        except Exception as e:\n",
    "            print(\"Error reading the file:\", e)\n",
    "    else:\n",
    "        print(\"File not found!\")\n",
    "\n",
    "    transcription_result = transcribe_audio(file_path)\n",
    "    print(\"Transcription Result:\", transcription_result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File exists: True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "file_path = \"/Users/munhuikim/Desktop/med-sync-be/test_audios/test01_20s.wav\"\n",
    "print(\"File exists:\", os.path.exists(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Post processing**\n",
    "1. Clean up the text (remove extra spaces, eliminating repeated words, fixing punctuation)\n",
    "2. Check spelling + auto correction\n",
    "3. Check grammar\n",
    "4. Use LLM (gpt) - not sure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note: the following text is the output of the day1_consultation01_doctor.wav\n",
    "\n",
    "'''raw_text = \"HELI I AND FRU STO YET A AN W A HAPPI HELPY TISS MORNINE YAN SO IT I HER THAT AND AND AND WE YOU STIN DIME IN AR WHATH YOU\n",
    "MEEN BY DIRAG YOU MEA NOU GANTS A TALLIN MOR OFTEN OR ARE YOUR STOOLS MOR LOOSE  CN AND HAVD ME TIMES TA DAY AR YOU GOIN I SA OR LAST COUPLOF \n",
    "DAYS SAIX SOME TIMES A DAN AND YOU MAN MENTIONIS  MAIN WAR TREE HAVEN NAIN YO THE INGS LIK BLOOD IN YOUR STOOLS ICAN AND YOU MENTIN YOV HAS \n",
    "SOME PAIN YOUR TOMY AS WELL WWHERE ABAUT AS A PAIN EXACTLY ONCE I AN WHAT SIDE IS THAT NET SI AGAN AND CAN YOU DESCRIME THE PAINTOME ICA AND IS TA PAIN\n",
    "IS THAT IS E TER ALL THE TIME TAT AS A COMING GO AA IS TA PAIN MOV ANYWHERE ELSE WITH ON PETWASE YOUR BACK ACAN NE AND YOU MENTION O E PENN QY WEE CAN \n",
    "SHAKEHERS WELL WOULD YOUOU BY SHAKE ID YOU MEAN YOU BEEN HAVING N AN YOU BEVENING FEVERISH FERSOMPLE  DOU MEATURE YOUR TEMPERTOR \n",
    "THEN AGA AC AND YOU OTHE SIMPE LIKE SWETTING OR A NIGHT\"'''\n",
    "\n",
    "# note: the following text is the output of the test01_20s.wav audio file\n",
    "raw_text = \"DANCING IN THE MASQUERADE IDLE TRUTH AN PLAIN SIGHT JADED POP ROLL CLICK SHOT WHO WILL I BE TO DAY OR NOT BUT SUCH A TIDE AS MOVING SEEMS ASLEEP TOO FULL FOR SOUND AND POAM WHEN THAT WHICH DREW FROM OUT THE BOUNDLESS DEEP TURNS AGAIN HOME TWILIGHT AND EVENING BELL AND AFTER THAT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text: DANCING IN THE MASQUERADE IDLE TRUTH AN PLAIN SIGHT JADED POP ROLL CLICK SHOT WHO WILL I BE TO DAY OR NOT BUT SUCH A TIDE AS MOVING SEEMS ASLEEP TOO FULL FOR SOUND AND POAM WHEN THAT WHICH DREW FROM OUT THE BOUNDLESS DEEP TURNS AGAIN HOME TWILIGHT AND EVENING BELL AND AFTER THAT\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def clean_transcription(text):\n",
    "    \"\"\"Enhanced cleaning: removes extra spaces, duplicate words, and noise.\"\"\"\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    text = re.sub(r'([.,!?])\\1+', r'\\1', text)  # Remove repeated punctuation\n",
    "    text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text)  # Remove duplicate words\n",
    "    \n",
    "    #text = re.sub(r'\\b[a-zA-Z]\\b', '', text)  # Remove single-character words (optional)\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers if they appear\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "cleaned_text = clean_transcription(raw_text)\n",
    "print(\"Cleaned Text:\", cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrected Text: DANCING IN THE MASQUERADE IDLE TRUTH AN PLAIN SIGHT JADED POP ROLL CLICK SHOT WHO WILL I BE TO DAY OR NOT BUT SUCH A TIDE AS MOVING SEEMS ASLEEP TOO FULL FOR SOUND AND POAM WHEN THAT WHICH DREW FROM OUT THE BOUNDLESS DEEP TURNS AGAIN HOME TWILIGHT AND EVENING BELL AND AFTER THAT\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "2. Spell checking + correction\n",
    "- Use SymSpell library to correct spelling mistakes in the transcription\n",
    "- Test whether it fixes mispelled words in the transcription\n",
    "\n",
    "'''\n",
    "# Install SymSpell if not installed\n",
    "# pip install symspellpy\n",
    "\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import requests\n",
    "import os\n",
    "\n",
    "dictionary_path = \"frequency_dictionary_en_82_765.txt\"\n",
    "if not os.path.exists(dictionary_path):\n",
    "    url = \"https://raw.githubusercontent.com/wolfgarbe/SymSpell/master/src/SymSpell/frequency_dictionary_en_82_765.txt\"\n",
    "    response = requests.get(url)\n",
    "    with open(dictionary_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "sym_spell = SymSpell(max_dictionary_edit_distance=2)\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "def correct_spelling(text):\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    \n",
    "    for word in words:\n",
    "        suggestions = sym_spell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        if suggestions:\n",
    "            corrected_words.append(suggestions[0].term)\n",
    "        else:\n",
    "            corrected_words.append(word)\n",
    "\n",
    "    return ' '.join(corrected_words)\n",
    "\n",
    "raw_text = \"HELPI TISS MORNINE YAN\"\n",
    "\n",
    "fixed_text = correct_spelling(cleaned_text)\n",
    "print(\"Corrected Text:\", fixed_text)  \n",
    "# Expected Output: \"HELP THIS MORNING YAN\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dancing in the MASQUERADE IDLE TRUTH an PLAIN SIGHT JADED POP ROLL CLICK SHOT. Who will I be to day or not but SUCH A TIDE AS MOVING SEEMS ASLEEP TOO FULL FOR SOUND AND POAM WHEN THAT WHICH DREW FROM OUT.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "3. Simple grammar correction\n",
    "Grammar correction model is HuggingFace\n",
    "It fixes bad sentence structures and improves readability automatically.\n",
    "'''\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "corrector = pipeline(\"text2text-generation\", model=\"facebook/bart-large-cnn\")\n",
    "\n",
    "def correct_grammar(text):\n",
    "    corrected_text = corrector(text, max_length=200)[0]['generated_text']\n",
    "    return corrected_text\n",
    "\n",
    "grammar_fixed_text = correct_grammar(raw_text)\n",
    "print(grammar_fixed_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n4. Use GPT to rewrite the transcription in proper English\\nSince the transcription is completely unreadable, \\nask llm to rewrite it into proper English.\\n\\nlimitation: privacy concerns, as the transcription may contain sensitive information.\\n'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "4. Use GPT to rewrite the transcription in proper English\n",
    "Since the transcription is completely unreadable, \n",
    "ask llm to rewrite it into proper English.\n",
    "\n",
    "limitation: privacy concerns, as the transcription may contain sensitive information.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Improved Transcription: Dancing in the MASQUERADE IDLE TRUTH an PLAIN SIGHT JADED POP ROLL CLICK SHOT. Who will I be to day or not but SUCH A TIDE AS MOVING SEEMS ASLEEP TOO FULL FOR SOUND AND POAM WHEN THAT WHICH DREW FROM OUT.\n"
     ]
    }
   ],
   "source": [
    "def improve_transcription(text):\n",
    "    text = clean_transcription(text)\n",
    "    text = correct_spelling(text)\n",
    "    text = correct_grammar(text)\n",
    "    return text\n",
    "\n",
    "better_transcription = improve_transcription(raw_text)\n",
    "print(\"Improved Transcription:\", better_transcription)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "whisper-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
